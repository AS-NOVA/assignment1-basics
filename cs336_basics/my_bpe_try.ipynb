{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d74748",
   "metadata": {},
   "source": [
    "在TinyStories上训练以字节为单位的BPE分词器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb80c7a",
   "metadata": {},
   "source": [
    "##### 预分词的并行化\n",
    "预分词步骤将成为一个重要的性能瓶颈，故可以使用built-in library `multiprocessing`并行化来加速。\n",
    "\n",
    "在并行化预分词的实现中，将语料分块时要注意块边界出现在special token的开始处。提供的预分词示例代码可以用于找到块的边界，找到后即可用于并行时的任务分配。这种分块策略永远是有用的，因为我们永远不会希望跨文档的合并操作。本作业中无需担心语料中没有`<|endoftext|>`导致块过大。\n",
    "\n",
    "##### 去掉special token\n",
    "在用`re.finditer`正则预分词之前，去掉special token（无论你处理整个语料还是某个块）。\n",
    "\n",
    "使用`re.split`和` \"|\" ⌋.join(special_tokens)`。(with careful use of re.escape since | may occur in the special tokens)\n",
    "\n",
    "这部分对应`test_train_bpe_special_tokens`。\n",
    "\n",
    "##### 优化合并步骤\n",
    "最朴素的合并算法太慢了，因为每次合并都要去看所有当前的字节对（或token对）。However, the only pair counts that change after each merge are those that overlap with the merged pair. Thus, BPE training speed can be improved by indexing the counts of all pairs and incrementally updating these counts, 而非一直遍历并计数所有对。这样能快很多，尽管这里不能并行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d8da4",
   "metadata": {},
   "source": [
    "##### 对于低算力：Profiling\n",
    "用`cProfile`或`scalene`等工具分析性能瓶颈并优化它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58110c",
   "metadata": {},
   "source": [
    "##### 对于低算力：Downscaling/降尺度\n",
    "先在数据集的一小部分上实验，例如在验证集上训练，大小约百分之一。选取小的子集时也要注意不能太小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795820e",
   "metadata": {},
   "source": [
    "### Problem (train_bpe): BPE Tokenizer Training (15 points)\n",
    "交付内容：写一个函数，输入文本文件路径，训练字节为单位的bpe分词器。\n",
    "\n",
    "输入参数：\n",
    "- `input_path`：`str`字符串，数据文本文件路径\n",
    "- `vocab_size`：`int`正整数，最终词表大小，包含最初的各种字节、合并出的内容、special tokens\n",
    "- `special_tokens`：`list[str]`字符串列表，不影响bpe训练\n",
    "\n",
    "返回参数：\n",
    "- `vocab`：`dict[int,bytes]`字节串的词典映射，每个字节串（即最终得到的token）编一个整数序号（即token ID）。\n",
    "- `merges`：`list[tuple[bytes, bytes]]`字节串二元组的列表，训练过程中合并token的记录，按合并顺序从前往后列出。\n",
    "\n",
    "最终目标：将写好的内容填入`adapters.py`文件中`run_train_bpe`函数处，运行`uv run pytest tests/test_train_bpe.py`进行测试。\n",
    "\n",
    "附加可选目标：把关键部分用cpp（用cppyy）或rust（用PyO3）来写。如果你要这么做，请注意区分哪些操作需要复制内存，哪些是直接从 Python 内存中读取。另外，请务必留下构建说明，或者确保项目仅使用 `pyproject.toml` 文件就能完成构建。\n",
    "\n",
    "另外注意给定的GPT-2的正则模板未必所有引擎中都支持，即使支持可能也很慢。已经验证`Oniguruma`库的速度相当快，并且支持负向先行断言（negative lookahead），但Python的`regex`也并不逊色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca8779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from cs336_basics.pretokenization_example import *\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0175c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7ff837746cf0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_io.BytesIO"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def path_to_bytesfile(p:str, n:int = -1)->BytesIO:\n",
    "    if n == -1:\n",
    "        with open(p, 'rb') as f:\n",
    "            text = f.read()\n",
    "        return BytesIO(text)\n",
    "    else:\n",
    "        with open(p, 'rb') as f:\n",
    "            text = f.read(n)\n",
    "        return BytesIO(text)\n",
    "    \n",
    "test = path_to_bytesfile(\"../data/TinyStoriesV2-GPT4-valid.txt\")\n",
    "print(test)\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e29929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5625758, 11252559, 16877372, 22502601]\n"
     ]
    }
   ],
   "source": [
    "limited_f = path_to_bytesfile(\"../data/TinyStoriesV2-GPT4-valid.txt\")\n",
    "\n",
    "# 定义分块参数并进行分块，获得边界\n",
    "num_processes = 4\n",
    "split_special_token = b\"<|endoftext|>\"\n",
    "boundaries = find_chunk_boundaries(limited_f, num_processes, split_special_token)\n",
    "print(boundaries)\n",
    "\n",
    "\n",
    "limited_f.seek(boundaries[0])\n",
    "chunk_bytes = limited_f.read(boundaries[1] - boundaries[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd42ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "<|endoftext|>\n",
      "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
      "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
      "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear t\n"
     ]
    }
   ],
   "source": [
    "# chunk_bytes = b\"asd|fadfasdf|a dfasdf<|endoftext|>adjfjf4564129d|w1dsvdsv|davniuab asdkvhiaudva<|endoftext|>ef,l;/a f.mbsop'fdbifadfdafdfa\"\n",
    "# chunk_bytes = b\"the cat on the mat\"\n",
    "chunk = chunk_bytes.decode('utf-8')\n",
    "print(chunk[:1000])  # 打印前1000个字符预览\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214169fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试文本前1000字符： u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "<|endoftext|>\n",
      "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
      "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
      "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear t\n",
      "输入的测试文本类型： <class 'str'>\n",
      "<\\|endoftext\\|>\n",
      "预分词结果前20个：\n",
      "['u', ' don', \"'t\", ' have', ' to', ' be', ' scared', ' of', ' the', ' loud', ' dog', ',', ' I', \"'ll\", ' protect', ' you', '\".', ' The', ' mole', ' felt']\n"
     ]
    }
   ],
   "source": [
    "# 对于每段文本，去掉特殊token并切分的过程\n",
    "def pre_tokenization_for_chunk(text_chunk:str, special_tokens: list[str]) -> list[str]:\n",
    "    escaped_special_tokens = [re.escape(t) for t in special_tokens]\n",
    "    escaped_special_tokens_in_one_str = \"|\".join(escaped_special_tokens)\n",
    "    print(escaped_special_tokens_in_one_str)\n",
    "    splited_text = re.split(escaped_special_tokens_in_one_str,text_chunk)\n",
    "    pre_tokenization = []\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    for doc in splited_text:\n",
    "        it = re.finditer(PAT,doc)\n",
    "        for match in it:\n",
    "            pre_tokenization.append(match.group())\n",
    "    return pre_tokenization\n",
    "\n",
    "\n",
    "print(\"测试文本前1000字符：\",chunk[:1000])\n",
    "print(\"输入的测试文本类型：\",type(chunk))\n",
    "\n",
    "res = pre_tokenization_for_chunk(chunk,[\"<|endoftext|>\"])\n",
    "print(\"预分词结果前20个：\")\n",
    "print(res[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405e80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以单一文档为例，整合预分词结果并转为bytes，前20个：\n",
      " [b'u', b' don', b\"'t\", b' have', b' to', b' be', b' scared', b' of', b' the', b' loud', b' dog', b',', b' I', b\"'ll\", b' protect', b' you', b'\".', b' The', b' mole', b' felt']\n"
     ]
    }
   ],
   "source": [
    "# 将多组预分词结果合并成一个列表，并转为bytes类型\n",
    "def get_all_pretoken_bytes(pre_tokens:list[list[str]])->list[bytes]:\n",
    "    res = [tok.encode(\"utf-8\") for trunks in pre_tokens for tok in trunks]\n",
    "    return res\n",
    "\n",
    "pre_tokens_bytes = get_all_pretoken_bytes([res])\n",
    "\n",
    "print(\"以单一文档为例，整合预分词结果并转为bytes，前20个：\\n\",pre_tokens_bytes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da3e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再写一遍测试文本，前20字节\n",
      " b\"u don't have to be s\"\n",
      "直接将bytes按字节翻译成整数\n",
      " [117, 32, 100, 111, 110, 39, 116, 32, 104, 97, 118, 101, 32, 116, 111, 32, 98, 101, 32, 115]\n"
     ]
    }
   ],
   "source": [
    "# 将一串bytes转为整数组\n",
    "def bytes_to_ints(input:bytes)->list[int]:\n",
    "    res = list(input)\n",
    "    return res\n",
    "\n",
    "print(\"再写一遍测试文本，前20字节\\n\",chunk_bytes[:20])\n",
    "print(\"直接将bytes按字节翻译成整数\\n\",bytes_to_ints(chunk_bytes[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b6a0c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试文本转为整数组，前十个pretoken\n",
      " [[117], [32, 100, 111, 110], [39, 116], [32, 104, 97, 118, 101], [32, 116, 111], [32, 98, 101], [32, 115, 99, 97, 114, 101, 100], [32, 111, 102], [32, 116, 104, 101], [32, 108, 111, 117, 100]]\n"
     ]
    }
   ],
   "source": [
    "pre_token_ints = [list(pre_token) for pre_token in pre_tokens_bytes]\n",
    "print(\"测试文本转为整数组，前十个pretoken\\n\",pre_token_ints[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10286d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "type IntPairPositionDict = dict[tuple[int,int],list[tuple[int,int]]]\n",
    "type IntBytesDict = dict[int,bytes]\n",
    "\n",
    "def create_token_pair_dict_int(pre_token_ints:list[list[int]]) -> IntPairPositionDict:\n",
    "    res = defaultdict(list)\n",
    "    for ptid, pt in enumerate(pre_token_ints):\n",
    "        for n in range(len(pt)-1):\n",
    "            res[(pt[n],pt[n+1])].append((ptid,n))\n",
    "    # return res\n",
    "    return dict(res) # 回到普通dict\n",
    "\n",
    "testdict = create_token_pair_dict_int(pre_token_ints)\n",
    "print(testdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BpeManager:\n",
    "    data: list[list[int]]\n",
    "    pos_dict: IntPairPositionDict\n",
    "    vocab_dict: IntBytesDict\n",
    "    merge_list:list[tuple[bytes,bytes]]\n",
    "    end: bool = False\n",
    "\n",
    "    def __init__(self, pre_token_ints:list[list[int]], special_tokens:list[str]) -> None:\n",
    "        self.data = pre_token_ints\n",
    "        self.pos_dict = create_token_pair_dict_int(pre_token_ints)\n",
    "        self.vocab_dict = {n: bytes([n]) for n in range(256)}\n",
    "        for st in special_tokens:\n",
    "            self.vocab_dict[len(self.vocab_dict)] = st.encode(\"utf-8\")\n",
    "        self.merge_list = []\n",
    "    \n",
    "    def get_max_token_pair_int(self) -> tuple[int,int]:\n",
    "        if not self.pos_dict:\n",
    "            # print(\"pos_dict为空，无法继续合并，结束合并过程\")\n",
    "            self.end = True\n",
    "            return (-1,-1)\n",
    "        maxpair = max(\n",
    "            self.pos_dict, \n",
    "            key = lambda k : (\n",
    "                len(self.pos_dict[k]), \n",
    "                self.vocab_dict[k[0]], \n",
    "                self.vocab_dict[k[1]]\n",
    "            )\n",
    "        )\n",
    "        # print(\"---寻找出现最多的token对---\\n\",\n",
    "        #     f\"出现最多的token对是{maxpair}，即{self.vocab_dict[maxpair[0]]}与{self.vocab_dict[maxpair[1]]}\\n\",\n",
    "        #     f\"出现了{len(self.pos_dict[maxpair])}次\\n\",\n",
    "        #     f\"出现在这些位置：{self.pos_dict[maxpair]}\\n\",\n",
    "        #     \"---寻找结束---\\n\")\n",
    "        return maxpair\n",
    "    \n",
    "    def clear_pos_dict(self,pos:tuple[int,int]):\n",
    "        # 清理第m个词的所有pair\n",
    "        m = pos[0]\n",
    "        zipped = zip(self.data[m][:-1],self.data[m][1:])\n",
    "        for pair in zipped :\n",
    "            for pairpos in self.pos_dict[pair]:\n",
    "                if pairpos[0] == m and pairpos in self.pos_dict[pair]:\n",
    "                    self.pos_dict[pair].remove(pairpos)\n",
    "            if self.pos_dict[pair] == []:\n",
    "                del self.pos_dict[pair]\n",
    "    \n",
    "    def rebuild_pos_dict(self,pos:tuple[int,int]):\n",
    "        # 填充第m个词的所有pair\n",
    "        m = pos[0]\n",
    "        zipped = zip(self.data[m][:-1],self.data[m][1:])\n",
    "        for i, pair in enumerate(zipped) :\n",
    "            self.pos_dict.setdefault(pair,[]).append((m,i))\n",
    "\n",
    "    # 一次merge的全过程\n",
    "    def merge(self, new_pair_id:tuple[int,int]) -> None:\n",
    "        # 在vocab_dict中加入新token nt = lt + rt\n",
    "        # 在merge中加入新merge (lt,rt)\n",
    "        li = new_pair_id[0]\n",
    "        ri = new_pair_id[1]\n",
    "        lt = self.vocab_dict[li]\n",
    "        rt = self.vocab_dict[ri]\n",
    "        ni = len(self.vocab_dict)\n",
    "        nt = lt + rt\n",
    "        self.vocab_dict[ni] = nt\n",
    "        self.merge_list.append((lt,rt))\n",
    "\n",
    "        # print(\"---开始合并---\\n\",\n",
    "        #     \"本次合并情况：\\n\",\n",
    "        #     f\"{li}--{lt}与{ri}--{rt}合并得到{nt}，编号为{ni}\")\n",
    "        \n",
    "        # 从pos_dict中找到所有 (li,ri) 的位置 (m,n)\n",
    "        assert (li,ri) in self.pos_dict\n",
    "        pos_list = self.pos_dict[(li,ri)]\n",
    "        # print(\"他们出现在这些位置：\",f\"{self.pos_dict[(li,ri)]}\")\n",
    "\n",
    "        while pos_list != []:\n",
    "            pos = pos_list.pop()\n",
    "            m = pos[0]\n",
    "            n = pos[1]\n",
    "            self.clear_pos_dict(pos)\n",
    "            # print(\"删改前的序列：\",self.data[m])\n",
    "            self.data[m].pop(n+1)\n",
    "            self.data[m][n] = ni\n",
    "            # print(\"删改后的序列：\",self.data[m])\n",
    "            self.rebuild_pos_dict(pos)\n",
    "\n",
    "        # print(\"---合并结束---\\n\")\n",
    "\n",
    "    def quick_look(self):\n",
    "        pass\n",
    "        # print(\"------状态速览------\")\n",
    "        # print(\"当前的data：\")\n",
    "        # print(self.data)\n",
    "        # # print([bytes(pt) for pt in self.data])\n",
    "        # print([[self.vocab_dict[i] for i in j] for j in self.data])\n",
    "        # print(\"当前的token对位置索引pos_dict：\")\n",
    "        # print(self.pos_dict)\n",
    "        # print([[(self.vocab_dict[k[0]],self.vocab_dict[k[1]]),v] for k,v in self.pos_dict.items()])\n",
    "        # print(\"当前的token表vocab_dict（最后5项）：\")\n",
    "        # print(list(self.vocab_dict.items())[-5:])\n",
    "        # print(\"当前的合并列表：\")\n",
    "        # print(self.merge_list)\n",
    "        # print(\"------状态速览------\")\n",
    "        # print()\n",
    "\n",
    "# testmanager = BpeManager(pre_token_ints,[\"testsptk\"])\n",
    "# testmanager.quick_look()\n",
    "\n",
    "# token_pair_id = testmanager.get_max_token_pair_int()\n",
    "# testmanager.merge(token_pair_id)\n",
    "# testmanager.quick_look()\n",
    "\n",
    "# token_pair_id = testmanager.get_max_token_pair_int()\n",
    "# testmanager.merge(token_pair_id)\n",
    "# testmanager.quick_look()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_bpe(\n",
    "    pre_token_ints: list[list[int]],\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str]\n",
    "    )->tuple[\n",
    "        dict[int,bytes],                # vocab\n",
    "        list[tuple[bytes,bytes]]        # merge\n",
    "    ]:\n",
    "\n",
    "    manager = BpeManager(pre_token_ints=pre_token_ints, special_tokens=special_tokens)\n",
    "\n",
    "    while len(manager.vocab_dict) < vocab_size:\n",
    "        print(\"当前词表大小：\",len(manager.vocab_dict))\n",
    "        # manager.quick_look()\n",
    "        maxpair = manager.get_max_token_pair_int()\n",
    "        print(\"获得的最大token对：\",\n",
    "              manager.vocab_dict[maxpair[0]],\n",
    "              manager.vocab_dict[maxpair[1]]\n",
    "              )\n",
    "        if manager.end:\n",
    "            print(\"合并过程提前结束\")\n",
    "            break\n",
    "        manager.merge(maxpair)\n",
    "\n",
    "    return (manager.vocab_dict, manager.merge_list)\n",
    "\n",
    "\n",
    "(vocab,merge) = main_bpe(pre_token_ints=pre_token_ints, vocab_size=5000, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(vocab.items())[-10:])\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bpe(input_path:str, vocab_size:int, special_tokens:list[str]):\n",
    "    # 从路径读取文件，转为文件指针。注意读出来的是bytes类型\n",
    "    limited_f = path_to_bytesfile(input_path, 4096)\n",
    "\n",
    "    # 定义分块参数并进行分块，获得边界\n",
    "    num_processes = 4\n",
    "    split_special_token = b\"<|endoftext|>\"\n",
    "    boundaries = find_chunk_boundaries(limited_f, num_processes, split_special_token)\n",
    "    print(boundaries)\n",
    "\n",
    "    \n",
    "    # limited_f.seek(4075)\n",
    "    # data = limited_f.read(100)\n",
    "    # print(data)\n",
    "\n",
    "    # 对每一块分别处理\n",
    "    for bound1, bound2 in zip(boundaries[:-1], boundaries[1:]):\n",
    "        #print(bound1, bound2)\n",
    "        limited_f.seek(bound1)\n",
    "        chunk = limited_f.read(bound2 - bound1) #每个chunk是bytes类型\n",
    "        print(len(chunk))\n",
    "    \n",
    "    # return vocab, merges\n",
    "\n",
    "my_bpe(\"../data/TinyStoriesV2-GPT4-valid.txt\", 100, [\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f56eb",
   "metadata": {},
   "source": [
    "课程提供的pretokenization用法\n",
    "```python\n",
    "with open(..., \"rb\") as f:\n",
    "    num_processes = 4\n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    # The following is a serial implementation, but you can parallelize this\n",
    "    # by sending each start/end pair to a set of processes.\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        # Run pre-tokenization on your chunk and store the counts for each pre-token\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae1529",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)\n",
    "**(a) 在 TinyStories 数据集上训练字节级 BPE 分词器。词表大小为10000。TinyStories的special token是<|endoftext|>。将训练生成的词表和合并序列化（serialize，即存成一个json之类的文件）到本地。训练过程花费了多少小时，占用了多少内存？词表中最长的词元（token）是什么？这个结果是否合理？**\n",
    "\n",
    "资源限制：不使用GPU情况下不超过30分钟，不超过30GB内存。\n",
    "\n",
    "提示：注意`<|endoftext|>`分割了各个文档。在进行bpe合并前要先处理它们。知道以上事实并使用并行预分词可以将时间压缩到两分钟内。\n",
    "\n",
    "交付内容：一到两句话。\n",
    "\n",
    "**(b)分析代码性能，哪一步耗时最多？** 交付内容：一到两句话。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a992e42",
   "metadata": {},
   "source": [
    "下面再在 `OpenWebText` 上训练试试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43812e",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)\n",
    "**(a) 同上题，数据集改为OpenWebText，词表大小改为32000。**\n",
    "\n",
    "资源限制：不使用GPU情况下不超过12小时，不超过100GB内存。\n",
    "\n",
    "交付内容：一到两句话。\n",
    "\n",
    "**(b) 比较两个数据集上训练的分词器的区别。** 交付内容：一到两句话。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
