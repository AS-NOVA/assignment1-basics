{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d74748",
   "metadata": {},
   "source": [
    "在TinyStories上训练以字节为单位的BPE分词器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb80c7a",
   "metadata": {},
   "source": [
    "##### 预分词的并行化\n",
    "预分词步骤将成为一个重要的性能瓶颈，故可以使用built-in library `multiprocessing`并行化来加速。\n",
    "\n",
    "在并行化预分词的实现中，将语料分块时要注意块边界出现在special token的开始处。提供的预分词示例代码可以用于找到块的边界，找到后即可用于并行时的任务分配。这种分块策略永远是有用的，因为我们永远不会希望跨文档的合并操作。本作业中无需担心语料中没有`<|endoftext|>`导致块过大。\n",
    "\n",
    "##### 去掉special token\n",
    "在用`re.finditer`正则预分词之前，去掉special token（无论你处理整个语料还是某个块）。\n",
    "\n",
    "使用`re.split`和` \"|\" ⌋.join(special_tokens)`。(with careful use of re.escape since | may occur in the special tokens)\n",
    "\n",
    "这部分对应`test_train_bpe_special_tokens`。\n",
    "\n",
    "##### 优化合并步骤\n",
    "最朴素的合并算法太慢了，因为每次合并都要去看所有当前的字节对（或token对）。However, the only pair counts that change after each merge are those that overlap with the merged pair. Thus, BPE training speed can be improved by indexing the counts of all pairs and incrementally updating these counts, 而非一直遍历并计数所有对。这样能快很多，尽管这里不能并行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d8da4",
   "metadata": {},
   "source": [
    "##### 对于低算力：Profiling\n",
    "用`cProfile`或`scalene`等工具分析性能瓶颈并优化它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58110c",
   "metadata": {},
   "source": [
    "##### 对于低算力：Downscaling/降尺度\n",
    "先在数据集的一小部分上实验，例如在验证集上训练，大小约百分之一。选取小的子集时也要注意不能太小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795820e",
   "metadata": {},
   "source": [
    "### Problem (train_bpe): BPE Tokenizer Training (15 points)\n",
    "交付内容：写一个函数，输入文本文件路径，训练字节为单位的bpe分词器。\n",
    "\n",
    "输入参数：\n",
    "- `input_path`：`str`字符串，数据文本文件路径\n",
    "- `vocab_size`：`int`正整数，最终词表大小，包含最初的各种字节、合并出的内容、special tokens\n",
    "- `special_tokens`：`list[str]`字符串列表，不影响bpe训练\n",
    "\n",
    "返回参数：\n",
    "- `vocab`：`dict[int,bytes]`字节串的词典映射，每个字节串（即最终得到的token）编一个整数序号（即token ID）。\n",
    "- `merges`：`list[tuple[bytes, bytes]]`字节串二元组的列表，训练过程中合并token的记录，按合并顺序从前往后列出。\n",
    "\n",
    "最终目标：将写好的内容填入`adapters.py`文件中`run_train_bpe`函数处，运行`uv run pytest tests/test_train_bpe.py`进行测试。\n",
    "\n",
    "附加可选目标：把关键部分用cpp（用cppyy）或rust（用PyO3）来写。如果你要这么做，请注意区分哪些操作需要复制内存，哪些是直接从 Python 内存中读取。另外，请务必留下构建说明，或者确保项目仅使用 `pyproject.toml` 文件就能完成构建。\n",
    "\n",
    "另外注意给定的GPT-2的正则模板未必所有引擎中都支持，即使支持可能也很慢。已经验证`Oniguruma`库的速度相当快，并且支持负向先行断言（negative lookahead），但Python的`regex`也并不逊色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca8779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from cs336_basics.pretokenization_example import *\n",
    "import regex as re\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d52c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0段的前100个字节：\n",
      "b'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little '\n",
      "第1段的前100个字节：\n",
      "b'<|endoftext|>\\nOnce upon a time, in a green forest, there lived a big bear and a little bunny. They w'\n",
      "第2段的前100个字节：\n",
      "b'<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim loved to print pictures of ani'\n",
      "第3段的前100个字节：\n",
      "b'<|endoftext|>\\nOnce upon a time, there was a big purple cat named Tom. Tom lived in a little house wi'\n"
     ]
    }
   ],
   "source": [
    "def path_to_chunks_bytes(p:str, n_parallel: int) -> list[bytes] :\n",
    "    res = []\n",
    "    with open(p,\"rb\") as f:\n",
    "        num_processes = n_parallel\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start) #.decode(\"utf-8\", errors=\"ignore\")\n",
    "            res.append(chunk)\n",
    "    return res\n",
    "\n",
    "testchunks = path_to_chunks_bytes(p=\"../data/TinyStoriesV2-GPT4-valid.txt\",n_parallel=4)\n",
    "for i, chunk in enumerate(testchunks):\n",
    "    print(f\"第{i}段的前100个字节：\")\n",
    "    print(chunk[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214169fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第0段测试文本信息：------\n",
      "前100字符： b'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little '\n",
      "预分词结果前10个：\n",
      "['u', ' don', \"'t\", ' have', ' to', ' be', ' scared', ' of', ' the', ' loud']\n",
      "------第1段测试文本信息：------\n",
      "前100字符： b'<|endoftext|>\\nOnce upon a time, in a green forest, there lived a big bear and a little bunny. They w'\n",
      "预分词结果前10个：\n",
      "['\\n', 'Once', ' upon', ' a', ' time', ',', ' in', ' a', ' green', ' forest']\n",
      "------第2段测试文本信息：------\n",
      "前100字符： b'<|endoftext|>\\nOnce upon a time, there was a little boy named Tim. Tim loved to print pictures of ani'\n",
      "预分词结果前10个：\n",
      "['\\n', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little']\n",
      "------第3段测试文本信息：------\n",
      "前100字符： b'<|endoftext|>\\nOnce upon a time, there was a big purple cat named Tom. Tom lived in a little house wi'\n",
      "预分词结果前10个：\n",
      "['\\n', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' big']\n"
     ]
    }
   ],
   "source": [
    "# 对于每段文本，去掉特殊token并切分的过程\n",
    "def pre_tokenization_for_chunk(text_chunk_bytes:bytes, special_tokens: list[str]) -> list[str]:\n",
    "    text_chunk = text_chunk_bytes.decode(\"utf-8\")\n",
    "    escaped_special_tokens = [re.escape(t) for t in special_tokens]\n",
    "    escaped_special_tokens_in_one_str = \"|\".join(escaped_special_tokens)\n",
    "    # print(escaped_special_tokens_in_one_str)\n",
    "    splited_text = re.split(escaped_special_tokens_in_one_str,text_chunk)\n",
    "    pre_tokenization = []\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    for doc in splited_text:\n",
    "        it = re.finditer(PAT,doc)\n",
    "        for match in it:\n",
    "            pre_tokenization.append(match.group())\n",
    "    return pre_tokenization\n",
    "\n",
    "testpretok = []\n",
    "for i, chunk in enumerate(testchunks) : \n",
    "    print(f\"------第{i}段测试文本信息：------\")\n",
    "    print(\"前100字符：\",chunk[:100])\n",
    "    res = pre_tokenization_for_chunk(chunk,[\"<|endoftext|>\"])\n",
    "    print(\"预分词结果前10个：\")\n",
    "    print(res[:10])\n",
    "    testpretok.append(res)\n",
    "\n",
    "# print(\"总体预分词结果：\",testpretok[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405e80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整合所有预分词结果并转为bytes，前10个：\n",
      "[b'u',\n",
      " b' don',\n",
      " b\"'t\",\n",
      " b' have',\n",
      " b' to',\n",
      " b' be',\n",
      " b' scared',\n",
      " b' of',\n",
      " b' the',\n",
      " b' loud']\n"
     ]
    }
   ],
   "source": [
    "def get_all_pretoken_bytes_and_build_count_dict(pre_tokens:list[list[str]]) -> dict[bytes,int]:\n",
    "    res = {}\n",
    "    for trunks in pre_tokens:\n",
    "        for tok in trunks:\n",
    "            btok = tok.encode(\"utf-8\")\n",
    "            res[btok] = res.get(btok,0) + 1\n",
    "    return res\n",
    "\n",
    "pre_tokens_bytes_dict = get_all_pretoken_bytes_and_build_count_dict(testpretok)\n",
    "print(\"整合所有预分词结果并转为bytes，前10个：\")\n",
    "pprint(list(pre_tokens_bytes_dict.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6a0c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试文本转为整数组，前十个pretoken\n",
      "[(117,),\n",
      " (32, 100, 111, 110),\n",
      " (39, 116),\n",
      " (32, 104, 97, 118, 101),\n",
      " (32, 116, 111),\n",
      " (32, 98, 101),\n",
      " (32, 115, 99, 97, 114, 101, 100),\n",
      " (32, 111, 102),\n",
      " (32, 116, 104, 101),\n",
      " (32, 108, 111, 117, 100)]\n"
     ]
    }
   ],
   "source": [
    "pre_token_ints_dict = {tuple(pre_token):cnt for pre_token , cnt in pre_tokens_bytes_dict.items()}\n",
    "print(\"测试文本转为整数组，前十个pretoken\")\n",
    "pprint(list(pre_token_ints_dict.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10286d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32, 100),\n",
      " (100, 111),\n",
      " (111, 110),\n",
      " (39, 116),\n",
      " (32, 104),\n",
      " (104, 97),\n",
      " (97, 118),\n",
      " (118, 101),\n",
      " (32, 116),\n",
      " (116, 111)]\n"
     ]
    }
   ],
   "source": [
    "type IntsCount = dict[tuple[int,...],int]\n",
    "type IntPairInInts = dict[tuple[int,int],set[tuple[int,...]]]\n",
    "\n",
    "def build_inverted_index(pre_token_ints_dict:IntsCount) -> IntPairInInts:\n",
    "    inverted_index = {}\n",
    "    for key in pre_token_ints_dict.keys():\n",
    "        pairs = zip(key[:-1],key[1:])\n",
    "        for pair in pairs:\n",
    "            inverted_index.setdefault(pair,set()).add(key)\n",
    "    return inverted_index\n",
    "\n",
    "testinvindex = build_inverted_index(pre_token_ints_dict)\n",
    "pprint(list(testinvindex.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb7bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BpeManager:\n",
    "    data: IntsCount\n",
    "    inv_index: IntPairInInts\n",
    "    vocab_dict: dict[int,bytes]\n",
    "    merge_list:list[tuple[bytes,bytes]]\n",
    "\n",
    "    def __init__(self, pre_token_ints_dict:IntsCount, special_tokens:list[str]) -> None:\n",
    "        self.data = pre_token_ints_dict\n",
    "        self.inv_index = build_inverted_index(pre_token_ints_dict=pre_token_ints_dict)\n",
    "        self.vocab_dict = {n: bytes([n]) for n in range(256)}\n",
    "        for st in special_tokens:\n",
    "            self.vocab_dict[len(self.vocab_dict)] = st.encode(\"utf-8\")\n",
    "        self.merge_list = []\n",
    "    \n",
    "    # 单个单词中数出现次数\n",
    "    def get_int_pair_occur_in_ints(self,intpair:tuple[int,int], ints: tuple[int,...]) -> int:\n",
    "        count = 0\n",
    "        for i in range(len(ints) - 1):\n",
    "            if ints[i] == intpair[0] and ints[i+1] == intpair[1] :\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    # 多个单词中数出现次数\n",
    "    def get_int_pair_occur_times(self,intpair:tuple[int,int]) -> int:\n",
    "        sum = 0\n",
    "        for pretokints in self.inv_index[intpair]:\n",
    "            sum += self.get_int_pair_occur_in_ints(intpair,pretokints) * self.data[pretokints]\n",
    "        return sum\n",
    "\n",
    "    def get_max_token_pair_int(self) -> tuple[int,int]:\n",
    "        maxpair = max(\n",
    "            self.inv_index, \n",
    "            key = lambda k : (                          # 这里k代指self.inv_index的key中的每个元素，即一个整数对\n",
    "                self.get_int_pair_occur_times(k),       # 先按其出现次数排\n",
    "                self.vocab_dict[k[0]],                  # 再看第一位的bytes\n",
    "                self.vocab_dict[k[1]]                   # 最后看第二位的bytes\n",
    "            )\n",
    "        )\n",
    "        # print(\"---寻找出现最多的token对---\\n\",\n",
    "        #     f\"出现最多的token对是{maxpair}，即{self.vocab_dict[maxpair[0]]}与{self.vocab_dict[maxpair[1]]}\\n\",\n",
    "        #     f\"出现了{self.get_int_pair_occur_times(maxpair)}次\\n\",\n",
    "        #     f\"出现在这些pretoken中（前10个）：{list(self.inv_index[maxpair])[:10]}\\n\",\n",
    "        #     \"---寻找结束---\\n\")\n",
    "        return maxpair\n",
    "\n",
    "    def build_new_token(self,new_pair_id:tuple[int,int]) -> tuple[int,bytes,int,bytes,int,bytes]:\n",
    "        # 在vocab_dict中加入新token nt = lt + rt\n",
    "        # 在merge中加入新merge (lt,rt)\n",
    "        li = new_pair_id[0]\n",
    "        ri = new_pair_id[1]\n",
    "        lt = self.vocab_dict[li]\n",
    "        rt = self.vocab_dict[ri]\n",
    "        ni = len(self.vocab_dict)\n",
    "        nt = lt + rt\n",
    "        self.vocab_dict[ni] = nt\n",
    "        self.merge_list.append((lt,rt))\n",
    "        return (li,lt,ri,rt,ni,nt)\n",
    "\n",
    "    # 一次merge的全过程\n",
    "    def merge(self, new_pair_id:tuple[int,int]) -> None:\n",
    "        (li,lt,ri,rt,ni,nt) = self.build_new_token(new_pair_id=new_pair_id)\n",
    "        # print(\"---开始合并---\\n\",\n",
    "        #     \"本次合并情况：\\n\",\n",
    "        #     f\"{li}--{lt}与{ri}--{rt}合并得到{nt}，编号为{ni}\")\n",
    "        \n",
    "        # 从inv_index中找到所有包含 (li,ri) 的pretoken单词\n",
    "        # assert (li,ri) in self.inv_index , \"反向索引中未找到所合并的对\"\n",
    "        # print(\"他们出现在这些单词中（前10个）：\",f\"{list(self.inv_index[(li,ri)])[:10]}\")\n",
    "\n",
    "        # 合并过程\n",
    "        # 两件事：维护反向索引，更新data\n",
    "        # 注意：原始的data中的每种pretoken是必定互不相同的，并且永远不可能变得相同\n",
    "        # 实际上，同一序列是有可能拆分为两种不同的token的，例如cat = c + at = ca + t，但是在bpe中永远不需要考虑这种事情\n",
    "\n",
    "        # 取出(li,ri)对应的所有pretoken(...,xi,li,ri,yi,...)\n",
    "        #     对于每个pretoken(...xi,li,ri,yi,...)：\n",
    "        #         取出所有它包含的pair(zi,wi)（包括(li,ri)本身）\n",
    "        #         对于每个pair(zi,wi)：\n",
    "        #             找到它里面的那个pretoken(...,xi,li,ri,yi,...)\n",
    "        #             （即使这个pair对应的其他单词里也有(li,ri)，也没事，因为总会遍历到的）\n",
    "        #             删掉它！\n",
    "        #         新建一个pretoken(...,xi,ni,yi,...)，加入语料库，次数与原本相同\n",
    "        #         取出所有它包含的pair(zi,wi)\n",
    "        #         对于每个pair(zi,wi)：\n",
    "        #             加上(...,xi,ni,yi,...)\n",
    "        #         从语料库中删除这个pretoken(...xi,li,ri,yi,...)\n",
    "        #     删掉这个(li,ri)（删前确保里面所有的单词都已经被删掉了）\n",
    "\n",
    "        while self.inv_index[(li,ri)] :\n",
    "            nowpretok = self.inv_index[(li,ri)].pop()\n",
    "            n = self.data[nowpretok]\n",
    "            del self.data[nowpretok]\n",
    "            for pair in zip(nowpretok[:-1],nowpretok[1:]):\n",
    "                self.inv_index[pair].discard(nowpretok)\n",
    "            newpretok = self.get_new_pretok_from_oldpretok_and_pair(nowpretok,(li,ri),ni=ni)\n",
    "            self.data[newpretok] = n\n",
    "            for pair in zip(newpretok[:-1],newpretok[1:]):\n",
    "                self.inv_index.setdefault(pair,set()).add(newpretok)\n",
    "\n",
    "        # 清理inv_index中空的pair\n",
    "        for pair in list(self.inv_index.keys()):\n",
    "            if self.inv_index[pair] == set():\n",
    "                del self.inv_index[pair]\n",
    "\n",
    "        # print(\"---合并结束---\\n\")\n",
    "\n",
    "    def get_new_pretok_from_oldpretok_and_pair(self,oldpretok:tuple[int,...],pair:tuple[int,int],ni:int) -> tuple[int,...]:\n",
    "        # 注意处理pair出现多次的情况！\n",
    "        newpretok = ()\n",
    "        i = 0\n",
    "        while i < len(oldpretok):\n",
    "            if i < len(oldpretok) - 1 and (oldpretok[i],oldpretok[i+1]) == pair:\n",
    "                # 找到一个pair\n",
    "                newpretok += (ni,)\n",
    "                i += 2\n",
    "            else:\n",
    "                newpretok += (oldpretok[i],)\n",
    "                i += 1\n",
    "        return newpretok\n",
    "\n",
    "    def quick_look(self):\n",
    "        pass\n",
    "        # print(\"------状态速览------\")\n",
    "        # print(\"当前的data：\")\n",
    "        # print(self.data)\n",
    "        # print([bytes(pt) for pt in self.data])\n",
    "        # print([[self.vocab_dict[i] for i in j] for j in self.data])\n",
    "        # print(\"当前的token对位置索引inv_index：\")\n",
    "        # print(self.inv_index)\n",
    "        # print([[(self.vocab_dict[k[0]],self.vocab_dict[k[1]]),v] for k,v in self.inv_index.items()])\n",
    "        clear_output(wait=True)\n",
    "        display(f\"当前词表大小：{len(self.vocab_dict)}，最新token：{list(self.vocab_dict.items())[-1:]}\")\n",
    "        # print(\"当前的合并列表：\")\n",
    "        # print(self.merge_list)\n",
    "        # print(\"------状态速览------\")\n",
    "        # print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10c488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"当前词表大小：9999，最新token：[(9998, b' raincoat')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def main_bpe(\n",
    "    pre_token_ints_dict: IntsCount,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str]\n",
    "    )->tuple[\n",
    "        dict[int,bytes],                # vocab\n",
    "        list[tuple[bytes,bytes]]        # merge\n",
    "    ]:\n",
    "\n",
    "    manager = BpeManager(pre_token_ints_dict=pre_token_ints_dict, special_tokens=special_tokens)\n",
    "\n",
    "    while len(manager.vocab_dict) < vocab_size:\n",
    "        \n",
    "        manager.quick_look()\n",
    "        if manager.inv_index == {}:\n",
    "            print(\"不再有任何相邻token对，合并过程提前结束\")\n",
    "            break\n",
    "        maxpair = manager.get_max_token_pair_int()\n",
    "        # print(\"获得的最大token对：\",\n",
    "        #       manager.vocab_dict[maxpair[0]],\n",
    "        #       manager.vocab_dict[maxpair[1]]\n",
    "        #       )\n",
    "        manager.merge(maxpair)\n",
    "\n",
    "    return (manager.vocab_dict, manager.merge_list)\n",
    "\n",
    "\n",
    "(vocab,merge) = main_bpe(pre_token_ints_dict=pre_token_ints_dict, vocab_size=10000, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdf4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9990, b' sly'), (9991, b' sinking'), (9992, b' roles'), (9993, b' roamed'), (9994, b' ribbit'), (9995, b' respecting'), (9996, b' releasing'), (9997, b' refusing'), (9998, b' raincoat'), (9999, b' racers')]\n",
      "[(b' s', b'ly'), (b' s', b'inking'), (b' ro', b'les'), (b' ro', b'amed'), (b' ribb', b'it'), (b' res', b'pecting'), (b' rele', b'asing'), (b' ref', b'using'), (b' rain', b'coat'), (b' rac', b'ers')]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.items())[-10:])\n",
    "print(merge[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1ffb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"当前词表大小：9999，最新token：[(9998, b' raincoat')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终结果展示：\n",
      "词表大小： 10000\n",
      "词表最后10个： [(9990, b' sly'), (9991, b' sinking'), (9992, b' roles'), (9993, b' roamed'), (9994, b' ribbit'), (9995, b' respecting'), (9996, b' releasing'), (9997, b' refusing'), (9998, b' raincoat'), (9999, b' racers')]\n",
      "合并列表最后10个： [(b' s', b'ly'), (b' s', b'inking'), (b' ro', b'les'), (b' ro', b'amed'), (b' ribb', b'it'), (b' res', b'pecting'), (b' rele', b'asing'), (b' ref', b'using'), (b' rain', b'coat'), (b' rac', b'ers')]\n"
     ]
    }
   ],
   "source": [
    "def my_bpe(input_path:str, vocab_size:int, special_tokens:list[str]):\n",
    "    # 从路径读取文件，转为文件指针。注意读出来的是bytes类型\n",
    "    chunks_bytes = path_to_chunks_bytes(p=input_path, n_parallel=4)\n",
    "    # 对每段文本进行预分词\n",
    "    pre_tokens = []\n",
    "    for chunk in chunks_bytes:\n",
    "        pretok = pre_tokenization_for_chunk(chunk, special_tokens)\n",
    "        pre_tokens.append(pretok)\n",
    "    # 整合所有预分词结果并转为bytes类型的token，建立词频字典\n",
    "    pre_tokens_bytes_dict = get_all_pretoken_bytes_and_build_count_dict(pre_tokens)\n",
    "    # 转为整数组表示的token，建立词频字典\n",
    "    pre_token_ints_dict = {tuple(pre_token):cnt for pre_token , cnt in pre_tokens_bytes_dict.items()}\n",
    "    # 运行BPE算法\n",
    "    (vocab,merge) = main_bpe(pre_token_ints_dict=pre_token_ints_dict, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "    return (vocab,merge)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "(vocab,merge) = my_bpe(\"../data/TinyStoriesV2-GPT4-valid.txt\", 10000, [\"<|endoftext|>\"])\n",
    "print(\"最终结果展示：\")\n",
    "print(\"词表大小：\",len(vocab))\n",
    "print(\"词表最后10个：\",list(vocab.items())[-10:])\n",
    "print(\"合并列表最后10个：\",merge[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd7ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db1f56eb",
   "metadata": {},
   "source": [
    "课程提供的pretokenization用法\n",
    "```python\n",
    "with open(..., \"rb\") as f:\n",
    "    num_processes = 4\n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    # The following is a serial implementation, but you can parallelize this\n",
    "    # by sending each start/end pair to a set of processes.\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        # Run pre-tokenization on your chunk and store the counts for each pre-token\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae1529",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)\n",
    "**(a) 在 TinyStories 数据集上训练字节级 BPE 分词器。词表大小为10000。TinyStories的special token是<|endoftext|>。将训练生成的词表和合并序列化（serialize，即存成一个json之类的文件）到本地。训练过程花费了多少小时，占用了多少内存？词表中最长的词元（token）是什么？这个结果是否合理？**\n",
    "\n",
    "资源限制：不使用GPU情况下不超过30分钟，不超过30GB内存。\n",
    "\n",
    "提示：注意`<|endoftext|>`分割了各个文档。在进行bpe合并前要先处理它们。知道以上事实并使用并行预分词可以将时间压缩到两分钟内。\n",
    "\n",
    "交付内容：一到两句话。\n",
    "\n",
    "**(b)分析代码性能，哪一步耗时最多？** 交付内容：一到两句话。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a992e42",
   "metadata": {},
   "source": [
    "下面再在 `OpenWebText` 上训练试试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43812e",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)\n",
    "**(a) 同上题，数据集改为OpenWebText，词表大小改为32000。**\n",
    "\n",
    "资源限制：不使用GPU情况下不超过12小时，不超过100GB内存。\n",
    "\n",
    "交付内容：一到两句话。\n",
    "\n",
    "**(b) 比较两个数据集上训练的分词器的区别。** 交付内容：一到两句话。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
