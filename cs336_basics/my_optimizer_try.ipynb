{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb776db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        # 模型内部存储：学习率\n",
    "        defaults = {\"lr\": lr}\n",
    "        # 父类中存储：各个参数，学习率（还可以传入其他自己想要传入的内容，以字典形式）\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure: Optional[Callable] = None): # \n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5acf25be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.92365264892578\n",
      "260.3128662109375\n",
      "870.2637329101562\n",
      "1492.09423828125\n",
      "1492.0941162109375\n",
      "928.5170288085938\n",
      "372.0384826660156\n",
      "97.47351837158203\n",
      "16.72380828857422\n",
      "1.8582000732421875\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=200)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a041bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Tuple\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAdamW\u001b[39;00m(\u001b[43mtorch\u001b[49m.optim.Optimizer):\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m     29\u001b[39m                  params: Iterable[Tensor] | Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] | Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Tensor]], \n\u001b[32m     30\u001b[39m                  lr: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1e-3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m                  weight_decay:\u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.1\u001b[39m\n\u001b[32m     35\u001b[39m                  ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lr < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m weight_decay < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m eps < \u001b[32m0\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# class SGD(torch.optim.Optimizer):\n",
    "#     def __init__(self, params, lr=1e-3):\n",
    "#         if lr < 0:\n",
    "#             raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "#         # 模型内部存储：学习率\n",
    "#         defaults = {\"lr\": lr}\n",
    "#         # 父类中存储：各个参数，学习率（还可以传入其他自己想要传入的内容，以字典形式）\n",
    "#         super().__init__(params, defaults)\n",
    "        \n",
    "#     def step(self, closure: Optional[Callable] = None): # \n",
    "#         loss = None if closure is None else closure()\n",
    "#         for group in self.param_groups:\n",
    "#             lr = group[\"lr\"] # Get the learning rate.\n",
    "#             for p in group[\"params\"]:\n",
    "#                 if p.grad is None:\n",
    "#                     continue\n",
    "#                 state = self.state[p] # Get state associated with p.\n",
    "#                 t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "#                 grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "#                 p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "#                 state[\"t\"] = t + 1 # Increment iteration number.\n",
    "#         return loss\n",
    "\n",
    "from typing import Any, Dict, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, \n",
    "                 params: Iterable[Tensor] | Iterable[Dict[str, Any]] | Iterable[Tuple[str, Tensor]], \n",
    "                 lr: float = 1e-3,\n",
    "                 beta1:float = 0.9,\n",
    "                 beta2:float = 0.95,\n",
    "                 eps:float = 1e-8,\n",
    "                 weight_decay:float = 0.1\n",
    "                 ) -> None:\n",
    "        if lr < 0 or weight_decay < 0 or eps < 0:\n",
    "            raise ValueError(f\"these params should be positive, \\\n",
    "                             now lr: {lr}, weight decay:{weight_decay},\\\n",
    "                             eps: {eps}\")\n",
    "        if (beta1 >= 1 or beta1 <= 0) or (beta2 >= 1 or beta2 <= 0):\n",
    "            raise ValueError(f\"beta1 and beta2 should be in (0,1), now: {beta1}, {beta2}\")\n",
    "        defaults = {\n",
    "            \"lr\":lr,\n",
    "            \"beta1\":beta1,\n",
    "            \"beta2\":beta2,\n",
    "            \"eps\":eps,\n",
    "            \"weight_decay\":weight_decay\n",
    "        }\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[Callable] = None): \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta1 = group[\"beta1\"]\n",
    "            beta2 = group[\"beta2\"]\n",
    "            eps = group[\"eps\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                g = p.grad  # 获得梯度\n",
    "                state = self.state[p] # 获得其他信息\n",
    "                # self.state是一个字典\n",
    "                # 其中每个kv对是 参数-参数信息，参数信息也是一个字典\n",
    "                # 参数信息可以随自己心意存储，例如\"t\"存储时间步\n",
    "\n",
    "                #最初：m和v为0，t没有\n",
    "                # 当伪代码中t=1的循环时，做的事：\n",
    "                # 0. 计算梯度\n",
    "                # 1. 更新m（用0作为初值）；更新v（用0作为初值）\n",
    "                # 2. 用当前的t=1来计算当前的学习率\n",
    "                # 3. 用当前学习率、当前m和当前v更新参数\n",
    "                # 4. 用权重衰减更新参数\n",
    "                # 5. t自增1\n",
    "\n",
    "                t = state.get(\"t\", 1)\n",
    "\n",
    "                m = state.get(\"first_moment\",torch.zeros_like(p)) # 第一次循环时先置0再更新，后续循环都是获取上一轮值再更新\n",
    "                m = beta1 * m + (1 - beta1) * g\n",
    "                v = state.get(\"second_moment\",torch.zeros_like(p))\n",
    "                v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "                lr_t = lr * math.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "\n",
    "                p -= lr_t * m / (torch.sqrt(v) + eps)\n",
    "                p -= lr * weight_decay * p\n",
    "\n",
    "                state[t] = t + 1\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1053dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3dddefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.4944)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.to(torch.float)\n",
    "torch.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f68d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   4,   9],\n",
       "        [ 16,  25,  36,  49],\n",
       "        [ 64,  81, 100, 121]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9d014b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.5000,  1.3333,  2.2500],\n",
       "        [ 3.2000,  4.1667,  5.1429,  6.1250],\n",
       "        [ 7.1111,  8.1000,  9.0909, 10.0833]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ** 2 / (x + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
