{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b099c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import einsum, rearrange, reduce, pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7884f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 in_features:int, \n",
    "                 out_features:int, \n",
    "                 device:torch.device | None = None,\n",
    "                 dtype:torch.dtype | None = None\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        # in out维度记录在模块内部\n",
    "        # tensor位置和类型信息并不属于模块，而是跟着tensor走\n",
    "        # 所以只是传给创建tensor的函数，需要这些信息时直接问tensor而非模块\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        parameter_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        # 创建一块参数矩阵\n",
    "        # 由于pytorch存储参数时是按行的，每行内容都会连在一起\n",
    "        # 而进行乘法时一定是长为in_features的那一维去乘输入向量\n",
    "        # 所以一定是让in_features作为行的长度，out_features作为行的数量\n",
    "        self.W = nn.Parameter(\n",
    "            torch.empty(\n",
    "                out_features,\n",
    "                in_features,\n",
    "                **parameter_kwargs    # 注意输入的设备和类型信息传到了这里！\n",
    "            ))\n",
    "        \n",
    "        # 初始化：方差为2/(d_in + d_out)，截断处在3个标准差\n",
    "        var = 2.0 / (in_features + out_features)\n",
    "        std = var ** 0.5\n",
    "        nn.init.trunc_normal_(self.W, std=std, a=-3*std, b=3*std)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # x的形状是 (..., d_in)\n",
    "        # W的形状是 (d_out, d_in)\n",
    "        # 输出形状是 (..., d_out)\n",
    "        # 简单写法：return x @ self.W.T\n",
    "        return einsum(x, self.W, \"... d_in, d_out d_in -> ... d_out\")   #和上面的等价\n",
    "        # 注意：x前面可能有许多维度，但最后一维一定是输入维度d_in\n",
    "        # 而为了方便计算，我们的W是d_out*d_in的\n",
    "        # 所以正常需要转置W才能相乘！这里用einsum来指定怎么乘，可以避免手动转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(4, 3)\n",
    "# 测试如何取到模块的所在设备\n",
    "print(linear.W.device)  # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_embeddings:int, # 词表大小\n",
    "                 embedding_dim:int, # 隐藏空间大小\n",
    "                 device:torch.device | None = None,\n",
    "                 dtype:torch.dtype | None = None\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_matrix = nn.Parameter(\n",
    "            torch.empty(\n",
    "                num_embeddings,\n",
    "                embedding_dim,\n",
    "                device=device,\n",
    "                dtype=dtype\n",
    "            )\n",
    "        )\n",
    "        nn.init.trunc_normal_(\n",
    "            self.embedding_matrix,\n",
    "            a = -3,\n",
    "            b = 3\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding_matrix[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b95ad",
   "metadata": {},
   "source": [
    "Deliverable: Implement RMSNorm as a torch.nn.Module. We recommend the following interface:\n",
    "def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None)\n",
    "Construct the RMSNorm module. This function should accept the following parameters:\n",
    "d_model: int Hidden dimension of the model\n",
    "eps: float = 1e-5 Epsilon value for numerical stability\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor Process an input tensor of shape\n",
    "(batch_size, sequence_length, d_model) and return a tensor of the same shape.\n",
    "Note: Remember to upcast your input to torch.float32 before performing the normalization (and\n",
    "later downcast to the original dtype), as described above.\n",
    "To test your implementation, implement the test adapter at [adapters.run_rmsnorm]. Then, run uv\n",
    "run pytest -k test_rmsnorm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cafb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 eps: float = 1e-5, \n",
    "                 device:torch.device | None = None, \n",
    "                 dtype:torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        parameter_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        self.gain_parameter = nn.Parameter(\n",
    "            torch.empty(\n",
    "                d_model,\n",
    "                **parameter_kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        nn.init.ones_(self.gain_parameter)\n",
    "\n",
    "        # 其实可以用torch.ones直接初始化，不过这里拆成两部分便于理解\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        rms = torch.sqrt(reduce(x**2,\"... d -> ... 1\",\"mean\") + self.eps) ** -1 \n",
    "        # 将x的最后一维压缩为一个标量作为分母，但是保留这一维度，便于广播\n",
    "        res = x * rms * self.gain_parameter\n",
    "        # 第一个乘法是因为手动保留了维度所以才能进行的，第二个乘法会自动广播\n",
    "        return res.to(in_dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "n = 3\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee544152",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sqrt(torch.sum(x**2) / 3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([4,5,6])\n",
    "w = x * y\n",
    "print(w)\n",
    "w = einsum(x,y,\"dim1, dim1 -> dim1\")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]]).to(torch.float32)\n",
    "y = torch.tensor([4,5,6]).to(torch.float32)\n",
    "z = einsum(x, y, \"a b, b -> a b\")\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1,2,3,1],\n",
    "                   [4,5,6,1],\n",
    "                   [1,4,7,11]],\n",
    "\n",
    "                  [[7,8,9,1],\n",
    "                   [10,11,12,1],\n",
    "                   [2,5,8,12]]])\n",
    "x = x.to(torch.float32)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17468b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = torch.sqrt(reduce(x**2, \"b s d -> b s 1\", \"mean\") + 0.25)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6290fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x / rms\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.tensor([1,2,3,4])\n",
    "z = x * g\n",
    "g1 = rearrange(g, \"d -> 1 1 d\")\n",
    "z1 = x * g1\n",
    "z2 = einsum(x, g, \"b s d, d -> b s d\")\n",
    "print(z)\n",
    "print()\n",
    "print(z1)\n",
    "print()\n",
    "print(z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.ones_(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = x * 8 / 3\n",
    "int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b156ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x:torch.Tensor) -> torch.Tensor:\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "testtensor = torch.tensor([[[1,2,3,1],\n",
    "                   [4,5,6,1],\n",
    "                   [1,4,7,11]],\n",
    "\n",
    "                  [[7,8,9,1],\n",
    "                   [10,11,12,1],\n",
    "                   [2,5,8,12]]])\n",
    "\n",
    "x = testtensor.clone()\n",
    "print(x)\n",
    "print(swish(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ac631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x:torch.Tensor) -> torch.Tensor:\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model:int,\n",
    "                 d_ff:int,\n",
    "                 device:torch.device | None = None,\n",
    "                 dtype:torch.dtype | None = None) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        parameter_kwargs = {\"device\":device,\"dtype\":dtype}\n",
    "\n",
    "        self.linear1 = Linear(d_model,d_ff,**parameter_kwargs)\n",
    "        self.linear2 = Linear(d_ff,d_model,**parameter_kwargs)\n",
    "        self.linear3 = Linear(d_model,d_ff,**parameter_kwargs)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:   \n",
    "        activates = swish(self.linear1(x))\n",
    "        gates = self.linear3(x)\n",
    "        return self.linear2(activates * gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = 8\n",
    "halfdq = int(dq/2)\n",
    "maxlen = 10\n",
    "THETA = 10000\n",
    "\n",
    "import math\n",
    "S = math.pow(THETA,-2/dq)\n",
    "print(S)\n",
    "\n",
    "thetas = [[i * math.pow(S,k-1) for k in range(1,halfdq+1)] for i in range(1,maxlen+1)]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "thetas = np.array(thetas)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(thetas)\n",
    "plt.imshow(thetas)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "itorch = torch.arange(1,maxlen+1)\n",
    "print(itorch)\n",
    "print(itorch.dtype, itorch.shape)\n",
    "rearrange(itorch,\"seq -> seq 1\")\n",
    "print(itorch)\n",
    "print(itorch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "ktorch = S ** torch.arange(halfdq)\n",
    "thetastorch = einsum(itorch,ktorch,\"seq, dimq -> seq dimq\")\n",
    "print(thetastorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 3\n",
    "d2 = 5\n",
    "d3 = 8\n",
    "testtorch = rearrange(torch.arange(d1*d2*d3),\"(d1 d2 d3)->d1 d2 d3\",d1 = d1,d2=d2,d3=d3)\n",
    "print(testtorch)\n",
    "print(testtorch.dtype, testtorch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = rearrange(testtorch,\"d1 d2 (c2 half) -> d1 d2 half c2\",c2=2)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "split[:,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f18027",
   "metadata": {},
   "source": [
    "theta: float Θ value for the RoPE\n",
    "d_k: int dimension of query and key vectors\n",
    "max_seq_len: int Maximum sequence length that will be inputted\n",
    "device: torch.device | None = None Device to store the buffer on\n",
    "\n",
    "def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor\n",
    "\n",
    "Process an input tensor of shape (..., seq_len, d_k) and return a tensor of the same shape.\n",
    "Note that you should tolerate x with an arbitrary number of batch dimensions. You should\n",
    "assume that the token positions are a tensor of shape (..., seq_len) specifying the token\n",
    "positions of x along the sequence dimension.\n",
    "You should use the token positions to slice your (possibly precomputed) cos and sin tensors\n",
    "along the sequence dimension.\n",
    "To test your implementation, complete [adapters.run_rope] and make sure it passes uv run\n",
    "pytest -k test_rope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a56b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    cosines: torch.Tensor\n",
    "    sines: torch.Tensor\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None) -> None:\n",
    "        # 初始化函数只需要负责根据输入的构建并存储起所有的正余弦值，便于使用\n",
    "        super().__init__()\n",
    "        assert d_k%2==0 , \"dimension of Q and K should be even for RoPE\"\n",
    "        halfd = int(d_k / 2)\n",
    "\n",
    "        positions = torch.arange(1,max_seq_len + 1, device=device) # 序列位置参数：1~maxlen\n",
    "        S = math.pow(theta,-2/d_k)\n",
    "        thetas = torch.pow(S,torch.arange(halfd,device=device)) # 所以theta的指数从0~大约-1\n",
    "        thetas_with_position = einsum(positions,thetas,\"maxlen, halfdk -> maxlen halfdk\")\n",
    "\n",
    "        self.register_buffer(\"cosines\", \n",
    "                             torch.cos(thetas_with_position),   \n",
    "                             persistent=False)\n",
    "        self.register_buffer(\"sines\", \n",
    "                             torch.sin(thetas_with_position),\n",
    "                             persistent=False)\n",
    "    \n",
    "        self.sines\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        # 先把输入的最后一位按奇偶分开，分别进行乘法后重新进行线性组合\n",
    "        # 使用sin和cos值时注意截断\n",
    "        rearranged_x = rearrange(x,\"... len (halfdk c2) -> ... len halfdk c2\",c2=2)\n",
    "        oddx = rearranged_x[1] # ... len halfdk\n",
    "        evenx = rearranged_x[0] # ... len halfdk\n",
    "        cut_cosines = self.cosines[token_positions] #\n",
    "        cut_sines = self.sines[token_positions]\n",
    "        # 三角函数阵： halfdk         \n",
    "        rotated_oddx = einsum(oddx, self.cosines, \"... len halfdk, len halfdk -> ... len halfdk\") - \\\n",
    "                      einsum(evenx, self.sines, \"... len halfdk, len halfdk -> ... len halfdk\")\n",
    "        rotated_evenx = einsum(evenx, self.cosines, \"... len halfdk, len halfdk -> ... len halfdk\") + \\\n",
    "                       einsum(oddx, self.sines, \"... len halfdk, len halfdk -> ... len half dk\")\n",
    "        res = rearrange(pack([rotated_evenx, rotated_oddx], \" ... len halfdk *\")[0],\n",
    "                        \"... len halfdk c2 -> ... len (halfdk c2)\")\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3dfa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n",
      "tensor([[[0.8907, 0.0834, 0.4916],\n",
      "         [0.4165, 0.8122, 0.9902],\n",
      "         [0.8252, 0.5467, 0.5307],\n",
      "         [0.1978, 0.4191, 0.5772]],\n",
      "\n",
      "        [[0.6628, 0.8670, 0.1062],\n",
      "         [0.6607, 0.4346, 0.2545],\n",
      "         [0.5123, 0.8700, 0.0817],\n",
      "         [0.3729, 0.3809, 0.7405]]])\n",
      "tensor([[[0.6365, 0.0560, 0.7873],\n",
      "         [0.7644, 0.5188, 0.0126],\n",
      "         [0.6020, 0.6399, 0.0708],\n",
      "         [0.3007, 0.0652, 0.0684]],\n",
      "\n",
      "        [[0.7699, 0.5737, 0.0032],\n",
      "         [0.8280, 0.6983, 0.8861],\n",
      "         [0.6605, 0.4224, 0.7843],\n",
      "         [0.8201, 0.7434, 0.0064]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[5.6693e-01, 4.6717e-03, 3.8707e-01],\n",
       "         [3.1837e-01, 4.2139e-01, 1.2439e-02],\n",
       "         [4.9682e-01, 3.4987e-01, 3.7585e-02],\n",
       "         [5.9471e-02, 2.7320e-02, 3.9462e-02]],\n",
       "\n",
       "        [[5.1025e-01, 4.9737e-01, 3.4173e-04],\n",
       "         [5.4706e-01, 3.0348e-01, 2.2548e-01],\n",
       "         [3.3839e-01, 3.6750e-01, 6.4079e-02],\n",
       "         [3.0581e-01, 2.8317e-01, 4.7356e-03]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.rand(2,4,3)\n",
    "print(t1.shape)\n",
    "t2 = torch.rand(2,4,3)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "einsum(t1,t2,\"a b c , a b c -> a b c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a38d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.arange(12)\n",
    "test = rearrange(test,\"(r c) -> r c\",r = 3)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54073463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE) layer.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    theta : float\n",
    "        Base used to generate inverse frequencies (e.g. 10_000).\n",
    "    d_k : int\n",
    "        Dimension of the key / query vectors (must be even).\n",
    "    max_seq_len : int\n",
    "        Maximum sequence length expected at inference / training time.\n",
    "    device : torch.device | None\n",
    "        Where to place the pre-computed sine / cosine tables.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 theta: float,\n",
    "                 d_k: int,\n",
    "                 max_seq_len: int,\n",
    "                 device=None):\n",
    "        super().__init__()\n",
    "        if d_k % 2 != 0:\n",
    "            raise ValueError(\"d_k must be even for RoPE.\")\n",
    "        self.d_k = d_k\n",
    "        # ---- pre-compute inverse frequencies ----\n",
    "        # freq[k] = 1 / theta ** (2k / d_k)          (k = 0,1,…,d_k/2-1)\n",
    "        freq = 1.0 / (theta ** (torch.arange(0,d_k,2, device=device).float() / d_k))\n",
    "\n",
    "        # shape: (max_seq_len, d_k // 2)\n",
    "        positions = torch.arange(max_seq_len, device=device).float()\n",
    "        freqs = torch.outer(positions, freq)\n",
    "\n",
    "        # cache cos/sin; no gradients needed → persistent=False\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs),persistent=False) # persistent=False does not save to state_dict\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs), persistent=False)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[Tensor, \"... seq_len d_k\"],\n",
    "        token_positions: Int[Tensor, \"... seq_len\"]\n",
    "        ) -> Float[Tensor, \"... seq_len d_k\"]:\n",
    "        \"\"\"\n",
    "        Apply RoPE to `x`.  Works with any batch shape prefix.\n",
    "        \"\"\"\n",
    "        # Check if the last dimension matches d_k\n",
    "        if x.size(-1) != self.d_k:\n",
    "            raise ValueError(f\"Last dim of x ({x.size(-1)}) ≠ d_k ({self.d_k}).\")\n",
    "        \n",
    "        # Gather the cached tables for the required positions\n",
    "        cos_pos = self.cos_cached[token_positions]\n",
    "        sin_pos = self.sin_cached[token_positions]\n",
    "\n",
    "        # Split even / odd channels\n",
    "        x_even = x[..., ::2]\n",
    "        x_odd = x[..., 1::2]\n",
    "\n",
    "        # Apply the 2-D rotation to each pair\n",
    "        out_even = x_even * cos_pos - x_odd * sin_pos\n",
    "        out_odd = x_even * sin_pos + x_odd * cos_pos\n",
    "\n",
    "        # Re-interleave\n",
    "        out = torch.empty_like(x)\n",
    "        out[..., ::2] = out_even\n",
    "        out[..., 1::2] = out_odd\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bbe1423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "y: tensor([[[24, 25, 26, 27],\n",
      "         [28, 29, 30, 31],\n",
      "         [32, 33, 34, 35]],\n",
      "\n",
      "        [[36, 37, 38, 39],\n",
      "         [40, 41, 42, 43],\n",
      "         [44, 45, 46, 47]]])\n",
      "z: tensor([[[ 0, 24,  1, 25,  2, 26,  3, 27],\n",
      "         [ 4, 28,  5, 29,  6, 30,  7, 31],\n",
      "         [ 8, 32,  9, 33, 10, 34, 11, 35]],\n",
      "\n",
      "        [[12, 36, 13, 37, 14, 38, 15, 39],\n",
      "         [16, 40, 17, 41, 18, 42, 19, 43],\n",
      "         [20, 44, 21, 45, 22, 46, 23, 47]]])\n"
     ]
    }
   ],
   "source": [
    "from einops import unpack\n",
    "\n",
    "x = torch.arange(2*3*4).reshape(2,3,4)\n",
    "y = torch.arange(2*3*4,2*2*3*4).reshape(2,3,4)\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "z = rearrange([x,y],\"two ... halfd -> ... (halfd two)\")\n",
    "print(\"z:\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01892413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 9])\n",
      "torch.Size([2, 2, 9])\n"
     ]
    }
   ],
   "source": [
    "(res1, res2) = unpack(z,[[1],[2]],\"d1 * d2\")\n",
    "print(res1.size())\n",
    "print(res2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325347d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0.0474, 0.0474, 0.0474],\n",
      "        [0.9526, 0.9526, 0.9526]])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    xlen = len(x.shape)\n",
    "    dim = dim % xlen\n",
    "    pat_origin = \" \".join([f\"d{i}\" for i in range(xlen)])\n",
    "    # print(pat_origin)\n",
    "    pat_reduce = \" \".join([f\"d{i}\" if i != dim else \"1\" for i in range(xlen)])\n",
    "    # print(pat_reduce)\n",
    "    x_max = reduce(x,f\"{pat_origin}->{pat_reduce}\",\"max\")\n",
    "    # print(x_max)\n",
    "    xminus = x - x_max\n",
    "    expx = torch.exp(xminus)\n",
    "    sum_expx = reduce(expx,f\"{pat_origin}->{pat_reduce}\",\"sum\")\n",
    "    res = expx / sum_expx\n",
    "    return res\n",
    "\n",
    "\n",
    "x = torch.arange(6).reshape(2,3)\n",
    "print(x)\n",
    "y = softmax(x,0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3d5eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.085536923187664\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.e ** 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f418d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jaxtyping\n",
    "from jaxtyping import Float, Bool, jaxtyped\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from beartype import beartype as typechecker\n",
    "\n",
    "Q : Float[Tensor, \"queries d_k\"] = torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
    "print(Q)\n",
    "\n",
    "@jaxtyped(typechecker=typechecker)\n",
    "def scaled_dot_product_attention(\n",
    "    Q: Float[Tensor, \" ... queries d_k\"],\n",
    "    K: Float[Tensor, \" ... keys d_k\"],\n",
    "    V: Float[Tensor, \" ... values d_v\"],\n",
    "    mask: Bool[Tensor, \" ... queries keys\"] | None = None,\n",
    ") -> Float[Tensor, \" ... queries d_v\"]:\n",
    "    d_k = Q.shape[-1]\n",
    "    scaled_prod = einsum(Q,K,\"... queries d_k, ... keys d_k -> ... queries keys\") \\\n",
    "                    / math.pow(d_k, 1/2)\n",
    "    if mask != None:\n",
    "        scaled_prod.masked_fill_(~mask, -torch.inf)\n",
    "\n",
    "    probs = softmax(scaled_prod,-1)\n",
    "    res = einsum(probs, V, \"... queries keys_also_values), ... keys_also_values d_v -> ... queries d_v\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaled_dot_product_attention(Q,Q,Q)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x:int|None):\n",
    "    x = x + 1\n",
    "    return x * x\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bffc61a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [False,  True,  True,  True]],\n",
      "\n",
      "        [[False, False,  True,  True],\n",
      "         [False,  True, False, False],\n",
      "         [ True,  True, False, False]]])\n",
      "tensor([[0.5654, 0.1289, 0.4903, 0.6791],\n",
      "        [0.2509, 0.6957, 0.2935, 0.1262],\n",
      "        [0.1683, 0.0790, 0.4932, 0.5859]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.1289, 0.4903, 0.0000],\n",
       "         [0.2509, 0.6957, 0.2935, 0.0000],\n",
       "         [0.0000, 0.0790, 0.4932, 0.5859]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.4903, 0.6791],\n",
       "         [0.0000, 0.6957, 0.0000, 0.0000],\n",
       "         [0.1683, 0.0790, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b: Bool[torch.Tensor,\"l m n\"] = torch.rand(2,3,4) > 0.5\n",
    "print(b)\n",
    "x: Float[torch.Tensor,\"m n\"] = torch.rand(3,4)\n",
    "print(x)\n",
    "einsum(b,x,\"l m n, m n -> l m n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "42452ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[-1.5638,  0.0540,  0.3095],\n",
      "         [ 2.4185,  0.7174,  1.3276]],\n",
      "\n",
      "        [[ 0.4948,  0.7561,  0.4457],\n",
      "         [ 0.4840,  1.2234,  1.1101]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 1.2251, -0.1811, -0.5047, -1.5832,  1.1824],\n",
      "         [-0.1172,  0.7591, -1.1259, -0.1950,  0.1354]],\n",
      "\n",
      "        [[ 0.7084, -0.1122,  0.3650,  0.0315,  0.6761],\n",
      "         [-1.8480,  1.5890, -0.2449,  0.2881, -2.0938]],\n",
      "\n",
      "        [[-0.1589,  0.5601, -0.8201,  1.4290,  0.1324],\n",
      "         [-1.5293, -0.7480, -1.0539,  1.4015,  0.4791]]])\n"
     ]
    }
   ],
   "source": [
    "@jaxtyped(typechecker=typechecker)\n",
    "def f(x:Float[Tensor,\"a b c\"],y:Float[Tensor,\"d e\"])->Float[Tensor,\"as a fj\"]:\n",
    "    z:Float[Tensor,\"as as\"] = torch.randn(2,3)\n",
    "    print(x.shape)\n",
    "    h:Float[Tensor,\"iof oifj fj\"]= torch.randn(3,2,5)\n",
    "    return h\n",
    "\n",
    "\n",
    "x:Float[Tensor,\"as as\"] = torch.randn(2,2,3)\n",
    "print(x.shape)\n",
    "y:Float[Tensor,\"b555 666\"] = torch.randn(3,2)\n",
    "\n",
    "print(x)\n",
    "\n",
    "try :\n",
    "    print(f(x,y))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb22ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 尝试运行成功示例 ---\n",
      "成功处理了一批数据，批次大小为: 10\n",
      "\n",
      "========================================\n",
      "\n",
      "--- 尝试运行失败示例 ---\n",
      "成功捕获到预期的错误:\n",
      "Type-check error whilst checking the parameters of __main__.process_data.\n",
      "The problem arose whilst typechecking parameter 'y'.\n",
      "Actual value: f32[5](torch)\n",
      "Expected type: <class 'Float[Tensor, 'batch']'>.\n",
      "----------------------\n",
      "Called with parameters: {'x': f32[10,3](torch), 'y': f32[5](torch)}\n",
      "Parameter annotations: (x: Float[Tensor, 'batch channels'], y: Float[Tensor, 'batch']) -> Any.\n",
      "The current values for each jaxtyping axis annotation are as follows.\n",
      "batch=10\n",
      "channels=3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from beartype import beartype\n",
    "from jaxtyping import jaxtyped, Float, install_import_hook\n",
    "\n",
    "# 使用 beartype 作为后端来装饰我们的函数，以进行运行时检查\n",
    "@jaxtyped(typechecker=beartype)\n",
    "def process_data(\n",
    "    x: Float[torch.Tensor, \"batch channels\"],\n",
    "    y: Float[torch.Tensor, \"batch\"],\n",
    "):\n",
    "    \"\"\"一个期望批次大小一致的函数。\"\"\"\n",
    "    print(f\"成功处理了一批数据，批次大小为: {x.shape[0]}\")\n",
    "\n",
    "# --- 1. 成功的例子 ---\n",
    "# 批次大小 (batch=10) 在两个张量中是一致的\n",
    "print(\"--- 尝试运行成功示例 ---\")\n",
    "try:\n",
    "    features_ok = torch.randn(10, 3)  # shape: (10, 3) -> batch=10, channels=3\n",
    "    labels_ok = torch.randn(10)       # shape: (10)    -> batch=10\n",
    "    process_data(features_ok, labels_ok)\n",
    "except Exception as e:\n",
    "    print(f\"出现错误: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# --- 2. 失败的例子 ---\n",
    "# 批次大小 (batch) 在两个张量中不一致 (10 vs 5)\n",
    "print(\"--- 尝试运行失败示例 ---\")\n",
    "try:\n",
    "    features_bad = torch.randn(10, 3) # shape: (10, 3) -> \"batch\" 被绑定为 10\n",
    "    labels_bad = torch.randn(5)       # shape: (5)    -> \"batch\" 尝试绑定为 5\n",
    "    # 下一行将抛出异常\n",
    "    process_data(features_bad, labels_bad)\n",
    "except Exception as e:\n",
    "    print(f\"成功捕获到预期的错误:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    # 这里多头注意力的算法参考原始transformer\n",
    "    # 所以dk = dv = dmodel / heads\n",
    "    def __init__(self, \n",
    "                 d_model:int,\n",
    "                 num_heads:int,\n",
    "                 max_seq_len:int,\n",
    "                 use_rope:bool = False,\n",
    "                 rope_theta:float = 10000.0,\n",
    "                 device:torch.device | None = None,\n",
    "                 dtype:torch.dtype | None = None\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.use_rope = use_rope\n",
    "        self.rope_theta = rope_theta\n",
    "        parameter_kwargs = {\"device\":device, \"dtype\":dtype}\n",
    "\n",
    "        if self.num_heads % self.d_model != 0:\n",
    "            raise ValueError(\"num_heads cant divide d_model\")\n",
    "        \n",
    "        self.d_head = int(self.d_model / self.num_heads)\n",
    "        self.linear_Wq = Linear(self.d_model, self.d_model, **parameter_kwargs)\n",
    "        self.linear_Wk = Linear(self.d_model, self.d_model, **parameter_kwargs)\n",
    "        self.linear_Wv = Linear(self.d_model, self.d_model, **parameter_kwargs)\n",
    "        self.linear_Wo = Linear(self.d_model, self.d_model, **parameter_kwargs)\n",
    "\n",
    "        self.pre_cache_mask =torch.tril(torch.ones(max_seq_len, max_seq_len)).bool()\n",
    "\n",
    "        if self.use_rope == True:\n",
    "            self.rope = RotaryPositionalEmbedding(self.rope_theta,\n",
    "                                                  self.d_head,\n",
    "                                                  self.max_seq_len)\n",
    "\n",
    "    def forward(self, \n",
    "                x:Float[Tensor,\"... len d_model\"]\n",
    "                )->Float[Tensor,\"... len d_model\"]:\n",
    "        len = x.shape[-2]\n",
    "        Q = self.linear_Wq(x)\n",
    "        K = self.linear_Wk(x)\n",
    "        V = self.linear_Wv(x)\n",
    "        Qs = rearrange(Q,\"... len (num_heads d_head) -> ... num_heads len d_head\")\n",
    "        Ks = rearrange(K,\"... len (num_heads d_head) -> ... num_heads len d_head\")\n",
    "\n",
    "        #对每个head进行相同的rope\n",
    "        if self.use_rope == True:\n",
    "            Qs = self.rope(Qs)\n",
    "            Ks = self.rope(Ks)\n",
    "        Vs = rearrange(V,\"... len (num_heads d_head) -> ... num_heads len d_head\")\n",
    "        mask = self.pre_cache_mask[:len,:len]\n",
    "        As = scaled_dot_product_attention(Qs,Ks,Vs,mask)\n",
    "        A = rearrange(As,\"... num_heads len d_head -> ... len (num_heads d_head)\")\n",
    "        res = self.linear_Wo(A)\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed6c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2,3,4)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aff06f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7464, 0.2385, 0.5180, 0.2581],\n",
      "         [0.6806, 0.3152, 0.3247, 0.5583],\n",
      "         [0.0113, 0.3023, 0.1931, 0.5379]],\n",
      "\n",
      "        [[0.6043, 0.5642, 0.8933, 0.7978],\n",
      "         [0.5209, 0.8047, 0.8264, 0.7800],\n",
      "         [0.5743, 0.6221, 0.8922, 0.3566]]])\n",
      "tensor([[[0.7464],\n",
      "         [0.6806],\n",
      "         [0.5379]],\n",
      "\n",
      "        [[0.8933],\n",
      "         [0.8264],\n",
      "         [0.8922]]])\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "xmax = einops.reduce(x,\"... v -> ... 1\",\"max\")\n",
    "print(x)\n",
    "print(xmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x:Float[Tensor,\"... len vocab_size\"],   # 省略了前置的batch，而这里实际上len也可以被看成一种batch\n",
    "                  real:Int[Tensor,\"... len\"]\n",
    "                 )->Float:\n",
    "    xmax = reduce(x,\"... v -> ... 1\",\"max\")\n",
    "    x -= xmax\n",
    "    expx = torch.exp(x)\n",
    "    logsumexps = torch.log(reduce(x,\"... v -> ... 1\",\"sum\"))\n",
    "\n",
    "\n",
    "    select = torch.gather(input=x,dim=-1,index=real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9d61a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8562, 0.6096, 0.0368, 0.7039],\n",
      "         [0.2631, 0.2487, 0.6048, 0.5304],\n",
      "         [0.8943, 0.8142, 0.7781, 0.4477]],\n",
      "\n",
      "        [[0.1969, 0.2993, 0.6578, 0.9369],\n",
      "         [0.4060, 0.7865, 0.8756, 0.8752],\n",
      "         [0.3288, 0.5966, 0.7848, 0.7925]]])\n",
      "tensor([[[1],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [2],\n",
      "         [1]]])\n",
      "tensor([[[0.6096],\n",
      "         [0.2487],\n",
      "         [0.7781]],\n",
      "\n",
      "        [[0.1969],\n",
      "         [0.8756],\n",
      "         [0.5966]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "select = torch.tensor([[1,1,2],[0,2,1]])\n",
    "import einops\n",
    "select = einops.rearrange(select,\"... -> ... 1\")\n",
    "gathered = torch.gather(x,dim=-1,index=select)\n",
    "print(x)\n",
    "print(select)\n",
    "print(gathered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53bc05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5969)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = einops.reduce(x,\"... -> \",\"mean\")\n",
    "ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
