{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720bd5eb",
   "metadata": {},
   "source": [
    "根据上文，我们已经能够训练bpe分词器，最终得到一个`merge:list[tuple[bytes,bytes]]`和一个`vocab:dict[int,bytes]`。现在要用这两个东西去编码和解码文本。\n",
    "\n",
    "编码过程：\n",
    "\n",
    "1. 预分词：文本->拆分成pretoken->每个pretoken转换为字节序列。注意这里不能再合并相同的pretoken了，必须按照顺序。\n",
    "\n",
    "2. 应用merge。根据训练时创造的merge序列，将它们逐个应用到每个pretoken身上，直到merge用完，或者pretoken已经变为了一整个token。\n",
    "\n",
    "- 注意事项1：编码时需要恰当地处理spetial tokens。我们的tokenizer需要允许用户额外输入新的special token，可以是原先训练分词器时没有的。这些token将会额外添加到词表vocab中。\n",
    "\n",
    "- 注意事项2：内存问题。如果要对一个很大的文本文件进行分词，没法全都读入到内存里，我们得想办法把它切分成块并且一块一块地处理。为此，我们需要在分块时保证一个token不能跨越块的边界，这样才能让分词结果和原本的方法相同。\n",
    "\n",
    "解码过程：\n",
    "\n",
    "只需要按照vocab把整数翻译成token，即字节序列，然后连在一起，然后用utf8解码。对不成字符的字节，用utf8默认的u+fffd替换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd8a8a",
   "metadata": {},
   "source": [
    "需要具备的方法：\n",
    "\n",
    "- 初始化：接受self，vocab，merge，special_tokens=None。创建分词器。\n",
    "\n",
    "- from_files：类方法，接受cls，vocab_filepath，merges_filepath，special_tokens=None，从文件读vocab和merges\n",
    "\n",
    "- encode：接受self和字符串text，将其转化为整数序列\n",
    "\n",
    "- encode_iterable：接受self和可迭代的多个str，返回可迭代的int，用于一次读不完所有文件的情况\n",
    "\n",
    "- decode：接受字符串序列，返回解码得到的str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self,\n",
    "                 vocab:dict[int,bytes],\n",
    "                 merges:list[tuple[bytes,bytes]],\n",
    "                 special_tokens:list[str]| None = None\n",
    "                 )-> None:\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls,\n",
    "                   vocab_filepath:str,\n",
    "                   merges_filepath:str,\n",
    "                   special_tokens:list[str]|None = None\n",
    "                   )->None:\n",
    "        pass\n",
    "\n",
    "    def encode(self,text:str) -> list[int]:\n",
    "        pass\n",
    "\n",
    "    def encode_iterable(self,\n",
    "                        iterable: Iterable[str]\n",
    "                        )->Iterator[int]:\n",
    "\n",
    "    def decode(self,ids:list[int]) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.mybpe import pre_tokenization_for_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the cat on the mat\"\n",
    "vocab = {\n",
    "\n",
    "}\n",
    "merge = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text:str) -> list[int]:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
