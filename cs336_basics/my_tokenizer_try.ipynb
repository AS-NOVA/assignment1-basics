{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720bd5eb",
   "metadata": {},
   "source": [
    "根据上文，我们已经能够训练bpe分词器，最终得到一个`merge:list[tuple[bytes,bytes]]`和一个`vocab:dict[int,bytes]`。现在要用这两个东西去编码和解码文本。\n",
    "\n",
    "编码过程：\n",
    "\n",
    "1. 预分词：文本->拆分成pretoken->每个pretoken转换为字节序列。注意这里不能再合并相同的pretoken了，必须按照顺序。\n",
    "\n",
    "2. 应用merge。根据训练时创造的merge序列，将它们逐个应用到每个pretoken身上，直到merge用完，或者pretoken已经变为了一整个token。\n",
    "\n",
    "- 注意事项1：编码时需要恰当地处理spetial tokens。我们的tokenizer需要允许用户额外输入新的special token，可以是原先训练分词器时没有的。这些token将会额外添加到词表vocab中。\n",
    "\n",
    "- 注意事项2：内存问题。如果要对一个很大的文本文件进行分词，没法全都读入到内存里，我们得想办法把它切分成块并且一块一块地处理。为此，我们需要在分块时保证一个token不能跨越块的边界，这样才能让分词结果和原本的方法相同。\n",
    "\n",
    "解码过程：\n",
    "\n",
    "只需要按照vocab把整数翻译成token，即字节序列，然后连在一起，然后用utf8解码。对不成字符的字节，用utf8默认的u+fffd替换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd8a8a",
   "metadata": {},
   "source": [
    "需要具备的方法：\n",
    "\n",
    "- 初始化：接受self，vocab，merge，special_tokens=None。创建分词器。\n",
    "\n",
    "- from_files：类方法，接受cls，vocab_filepath，merges_filepath，special_tokens=None，从文件读vocab和merges\n",
    "\n",
    "- encode：接受self和字符串text，将其转化为整数序列\n",
    "\n",
    "- encode_iterable：接受self和可迭代的多个str，返回可迭代的int，用于一次读不完所有文件的情况\n",
    "\n",
    "- decode：接受字符串序列，返回解码得到的str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50ae64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.mybpe import pre_tokenization_for_chunk\n",
    "from typing import Iterable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    \"\"\"\n",
    "    自制的bpe tokenizer类。指定vocab和merges以及一些特殊token后，能够对任意给定的文本进行编码，或对给定的id序列进行解码。\n",
    "    Attributes:\n",
    "        vocab (dict[int,bytes]) : 词汇表，按id查找token字节序列\n",
    "        _reverse_vocab (dict[bytes,int]) : 反词汇表，按token字节序列查找id\n",
    "        merges (list[tuple[bytes,bytes]]) : 训练过程中生成的merge序列\n",
    "        _merges_rank (dict[tuple[bytes,bytes],int]) : merge序号表，便于O(1)查找到任何merge的序号，判断其顺序\n",
    "        special_tokens (list[str]| None): 额外提供的特殊token，将来使用时必须将这些特殊token视为独立单位，而不能分割\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab:dict[int,bytes],\n",
    "                 merges:list[tuple[bytes,bytes]],\n",
    "                 special_tokens:list[str]| None = None\n",
    "                 )-> None:\n",
    "        \"\"\"\n",
    "        接收vocab、merges和特殊token序列以进行分词器的初始化。\n",
    "        初始化过程中会建立vocab的反表，并集合化merges，便于按输入内容进行encode。\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self._get_reverse_vocab()\n",
    "        self.merges = merges\n",
    "        self._get_merges_rank()\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def _get_reverse_vocab(self) -> dict[bytes,int]:\n",
    "        \"\"\"\n",
    "        返回self.vocab的逆表，即根据token字节序列查询其id。\n",
    "        如果没计算过逆表，会重新计算并存入self.reverse_vocab。否则直接返回。\n",
    "        Returns:\n",
    "            dict[bytes,int]: 当前vocab的逆表，可以根据token字节序列查询其id。\n",
    "        \"\"\"\n",
    "        # #print(self.vocab.items())\n",
    "        if not hasattr(self,\"reversed_vocab\"):\n",
    "            self._reverse_vocab = {tok:id for id, tok in self.vocab.items()}\n",
    "        return self._reverse_vocab\n",
    "\n",
    "    def _get_merges_rank(self) -> dict[tuple[bytes,bytes],int]:\n",
    "        \"\"\"\n",
    "        返回self.merges转为的根据merge内容查询序号的列表，以便进行O(1)的查找。\n",
    "        如果没有创建过哈希，会将其存入self._merges_rank。否则直接返回。\n",
    "        Returns:\n",
    "            dict[tuple[bytes,bytes],int]: 根据merge内容查找序号的列表。\n",
    "        \"\"\"\n",
    "        if not hasattr(self,\"_merges_rank\"):\n",
    "            self._merges_rank = dict([(merge, i) for (i, merge) in enumerate(self.merges)])\n",
    "        return self._merges_rank\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls,\n",
    "                   vocab_filepath:str,\n",
    "                   merges_filepath:str,\n",
    "                   special_tokens:list[str]|None = None\n",
    "                   )->None:\n",
    "        pass\n",
    "\n",
    "    def encode(self,text:str) -> list[int]:\n",
    "        \"\"\"\n",
    "        基于存储的vocab和merge序列，输入文本字符串，将其预分词，按utf-8编码解读并进行bpe分词，最后返回token id序列。\n",
    "        Arguments:\n",
    "            text (str): 字符串形式的输入文本\n",
    "        Returns:\n",
    "            list[int]: 一个token id列表，即对输入文本的预分词结果\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        pretoks = pre_tokenization_for_chunk(text,self.special_tokens,keep_special_tokens=True)\n",
    "        # print(\"预分词结果：\",pretoks)\n",
    "        pretoks_bytes = [pretok.encode(\"utf-8\") for pretok in pretoks]\n",
    "        for pretok_bytes in pretoks_bytes:\n",
    "            # #print(pretok_bytes)\n",
    "            ints = self._encode_for_bytes(pretok_bytes)\n",
    "            # #print(ints)\n",
    "            res += ints\n",
    "        #print(\"编码结果：\",res)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    def _encode_for_bytes(self,input:bytes) -> list[int]:\n",
    "        \"\"\"\n",
    "        对于给定的字节序列，进行bpe分词，即不断进行merge过程，最终返回得到的token id序列。\n",
    "        注意：这理应对任意字节序列都能运行，因为vocab中理应包含了所有单字节。即使一次merge也无法进行，也应该返回所有单字节本身的id。\n",
    "        另外，对于special token，它们无法通过merge合并出来，所以需要直接用vocab进行查询。\n",
    "        Arguments:\n",
    "            input (bytes): 任意的字节序列\n",
    "        Returns:\n",
    "            out (list[int]): bpe分词得到的token id列表。\n",
    "        \"\"\"\n",
    "        # 先找出特殊token\n",
    "        # ！！！！注意：这里认为特殊token一定在vocab里，一定有一个能查到的序号！！！！\n",
    "        if self.special_tokens != None and input.decode(\"utf-8\") in self.special_tokens:\n",
    "            if input not in self._get_reverse_vocab():\n",
    "                raise KeyError(f\"{input}不在词汇表中！\")\n",
    "            #print(f\"{input}是一个特殊token，编号为{self._get_reverse_vocab()[input]}\")\n",
    "            return [self._get_reverse_vocab()[input]]\n",
    "        else:\n",
    "            # TODO:编写不断进行merge的过程。\n",
    "            bytes_list = [bytes([b]) for b in input]\n",
    "            #print(f\"正在编码:{bytes_list}\")\n",
    "            while(len(bytes_list)>1):\n",
    "                # step 1: 找到所有merge中序号最小的。如果没有，说明不能继续merge，应该直接将当前的所有token转为数字\n",
    "                pairs_iter = zip(bytes_list[:-1],bytes_list[1:])\n",
    "                #print(f\"当前存在的token对：{[pair for pair in zip(bytes_list[:-1],bytes_list[1:])]}\")\n",
    "\n",
    "                valid_pairs = [pair for pair in pairs_iter if pair in self._get_merges_rank()]\n",
    "                #print(f\"当前可用于合并的token对：{valid_pairs}\")\n",
    "                min_pair = min(valid_pairs, \n",
    "                               key=lambda pair: self._merges_rank.get(pair, float(\"inf\")), # 其实已经滤除了不在merges里的，所以并不会真的有inf出现\n",
    "                               default=None)\n",
    "                if min_pair == None:\n",
    "                    #print(f\"找不到可用的合并，停止合并并直接编码\")\n",
    "                    res = self._get_id_for_bytes_list(bytes_list)\n",
    "                    return res\n",
    "                #print(f\"序号最小的merge：{min_pair}\")\n",
    "                # step 2: 如果找到了最小的对，就用它对bytes_list进行合并\n",
    "                new_tok = min_pair[0]+min_pair[1]\n",
    "                new_list = []\n",
    "                i=0\n",
    "                while i <= len(bytes_list)-1:\n",
    "                    if i < len(bytes_list)-1 and bytes_list[i] == min_pair[0] and bytes_list[i+1] == min_pair[1]:\n",
    "                        new_list.append(new_tok)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_list.append(bytes_list[i])\n",
    "                        i += 1\n",
    "                \n",
    "                # step 3: 合并后的新列表拿去继续循环\n",
    "                #print(f\"经过一轮merge，当前结果：{new_list}\")\n",
    "                bytes_list = new_list\n",
    "\n",
    "\n",
    "\n",
    "            # 退出循环只有一种可能，就是bytes_list长度为1了\n",
    "            #print(f\"merge结束后的结果：{bytes_list}\")\n",
    "            res = self._get_id_for_bytes_list(bytes_list)\n",
    "            #print(f\"对应到词汇表：{res}\")\n",
    "            return res\n",
    "    \n",
    "    def _get_id_for_bytes_list(self,bytes_list:list[bytes])->list[int]:\n",
    "        \"\"\"\n",
    "        把给定的字节形式的token列表翻译为token id列表。\n",
    "        Arguments:\n",
    "            bytes_list(list[bytes]): 输入的字节形式的token列表。使用时应当保证输入的每个字节串都是已知的token。\n",
    "        Returns:\n",
    "            out(list[int]): 将每个bytes翻译为token id的结果列表。\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for b in bytes_list:\n",
    "            if b not in self._reverse_vocab:\n",
    "                raise KeyError(f\"{b}不在词汇表中\")\n",
    "            res.append(self._reverse_vocab[b])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def encode_iterable(self,\n",
    "                        iterable: Iterable[str]\n",
    "                        )->Iterator[int]:\n",
    "        pass\n",
    "\n",
    "    def decode(self,ids:list[int]) -> str:\n",
    "        res = \"\"\n",
    "        for id in ids:\n",
    "            res += self.vocab[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af14dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the cat <|endoftext|> ate\"\n",
    "vocab = {0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b' at',100:b'<|endoftext|>'}\n",
    "merge = [(b't', b'h'), (b' ', b'c'), (b' ', b'a'), (b'th', b'e'), (b' a', b't')]\n",
    "special_tokens = [\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e673f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(b't', b'h'): 0, (b' ', b'c'): 1, (b' ', b'a'): 2, (b'th', b'e'): 3, (b' a', b't'): 4}\n",
      "{b' ': 0, b'a': 1, b'c': 2, b'e': 3, b'h': 4, b't': 5, b'th': 6, b' c': 7, b' a': 8, b'the': 9, b' at': 10, b'<|endoftext|>': 100}\n"
     ]
    }
   ],
   "source": [
    "# 测试反表\n",
    "a:MyTokenizer\n",
    "tokenizer = MyTokenizer(vocab, merge, special_tokens)\n",
    "print(tokenizer._merges_rank)\n",
    "print(tokenizer._reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96f1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(b't', b'h'): 0, (b' ', b'c'): 1, (b' ', b'a'): 2, (b'th', b'e'): 3, (b' a', b't'): 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 7, 1, 5, 0, 100, 10, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer._merges_rank)\n",
    "tokenizer.encode(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
